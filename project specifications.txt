Project Specification: Universal TTS API Wrapper
1. Project Objective
Develop a production-ready Flask wrapper for pocket-tts that supports synthesis, voice cloning, and streaming audio, compatible with standard industry API TTS schemas (OpenAI) and includes a simple but elegant preview website that runs on the same host/post as the TTS server endpoint .

2. Phase 1: Infrastructure & Configuration
Implement a robust CLI-driven configuration layer to handle model loading and hardware orchestration.

CLI Argument Parser: Support arguments for --model_path, --host (host to run server on), --port (post to run server on), and --stresm (if added, use streaming TTS)

3. Phase 2: Core Synthesis Logic
Develop necessary functions to bridge the API and the model to allow for streaming and non-streaming voice TTS generation.

Text Pre-processing: Integrate a text cleaner/tokenizer to handle language-specific normalization.

Audio Post-processing:

Function to convert raw model output (tensors) into a BytesIO buffer.

Use torchaudio to convert raw audio into various formats: mp3, wav, opus, aac, and flac.

Voice Cloning/Conditioning: Support accepting a path to a reference audio file for zero-shot cloning.  If supported by model, cache latents to avoid having to reprocess wav files for cloning each generarion.

4. Phase 3: API Surface Implementation
Expose the model to the user in two ways:

A. Simple web page at root of host/port used for server
Use pocket-tts-logo.png and develop a simple localhost html and or css web page that is run automatically when running the server and allows the user to test and play output from the built in voices or select a wav file from their file system to clone and provides from default text prompts as well as a free text input field the user can utilize to test the model, and download output wavs through a file selection dislog to a location of their choice.

B. OpenAI Compatibility Layer endpoint (/v1/audio/speech)
Endpoint: Follow OpenAIâ€™s JSON payload schema: { "model": "...", "input": "...", "voice": "...", "response_format": "mp3", "speed": 1.0 }.
The response format should support the formats noted above
Speed should be passed to the model if supported by pocket-tts to control how fast the TTS plays
The "voice" parameter should accept one of the model's built in voice names or the absolute path to a .wav file specified by the user
Model should always be defaulted to "pocket-tts" if not specified and should have no i,pact on the actual voice generation as this server is only for use with the pocket-tts model
Logic: Map "voice" strings to internal speaker IDs (for voices supported natively by the model) or paths to cloning .wav files.
Non-streaming response: If not using streaming via command line argument when running server, generate a non-streaming file response  supporting all the audio formats specified above
Streaming: Implement steaming audio response option at user's discretion using Response(generate_chunks(), mimetype=...).

Voices endooint: also include a voices endpoint that will return built in voices and the wav files in a directory for cloning wav files specified by the user as a command line argument when running the server:
Voices endpoint sample to adapt:

@app.route("/v1/voices", methods=["GET"])
def openai_list_voices():
    """JSON list of available built-in voice names.
    Not an official OpenAI endpoint; provided for convenience for clients.
    """
    voices = _get_builtin_voice_names()
    return jsonify(
        {
            "object": "list",
            "data": [{"id": v, "object": "voice"} for v in voices],
        }
    )

5. Phase 4: Error Handling: Implement try-except blocks around relevant logic to return 500-level JSON errors instead of crashing the Flask thread.

6. Technical Requirements
Language: Python 3.11

Framework: Flask

Core Libraries: torch, torchaudio, numpy, whatever else needed